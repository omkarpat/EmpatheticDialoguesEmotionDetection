{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baselines.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkarpat/EmpatheticDialoguesEmotionDetection/blob/master/Baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ed6RYn3sV9d0",
        "outputId": "d3c7d701-3bed-4498-fd09-1226d33d9ac6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# Colab settings/mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My\\ Drive/CSE\\ 245\\ Project"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/CSE 245 Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ybznvtw9WnuJ",
        "outputId": "d87229be-6a36-4d59-c723-b3c1e5a0582e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "!ls Data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Data Exploration.ipynb'\n",
            " data_fixed_train.json\n",
            " data_sample_100.json\n",
            " data_sample_10.json\n",
            " data_sample_10_processed.json\n",
            " data_sample_fixed_processed_final.json\n",
            " data_sample_fixed_processed_model1_final.csv\n",
            " data_sample_fixed_processed_model2_final.csv\n",
            " data_sample_fixed_processed_model3_final.csv\n",
            " data_sample_fixed_processed_model4_final.csv\n",
            " fixed\n",
            " fixed_test.json\n",
            " fixed_train_516.csv\n",
            " fixed_valid.json\n",
            "'informative words.ipynb'\n",
            " Raw\n",
            " test_fixed_processed_model1_final.csv\n",
            " test_fixed_processed_model2_final.csv\n",
            " test_fixed_processed_model3_final.csv\n",
            " test_fixed_processed_model4_final.csv\n",
            " valid.csv\n",
            " valid_fixed_processed_model1_final.csv\n",
            " valid_fixed_processed_model2_final.csv\n",
            " valid_fixed_processed_model3_final.csv\n",
            " valid_fixed_processed_model4_final.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G-95D-Lm_PUt",
        "outputId": "eaf41427-8a7b-49ec-8787-2a2913ac97b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "!pip install textblob\n",
        "!pip install flair"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.12.0)\n",
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/29/81e3c9a829ec50857c23d82560941625f6b42ce76ee7c56ea9529e959d18/flair-0.4.5-py3-none-any.whl (136kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/f9/9f2b6c672c8f8bb87a4c1bd52c1b57213627b035305aad745d015b2a62ae/pytest-5.4.2-py3-none-any.whl (247kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 7.5MB/s \n",
            "\u001b[?25hCollecting transformers>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 17.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Collecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 20.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.0+cu101)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.1.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.4)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.8.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (19.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.6.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (8.3.0)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 23.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (0.7)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 54.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.0.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.15.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.3.0->flair) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.3.0->flair) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.3.0->flair) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.3.0->flair) (2.9)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (1.13.13)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (1.16.13)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.15.2)\n",
            "Building wheels for collected packages: segtok, mpld3, sqlitedict, langdetect, sacremoses\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25020 sha256=0acc4533166f323474e47e20c888b3e70db9db8dde5afda4d3fd2eace40105a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=891a09429821d4f6006346b623eb78992625a859dda5b6c8e5652f45619dbb12\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=935ef32d717e51695f18a5777822605c3692d607c20a345c5c92156c837d76ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=42df173a541459c03fba44a55dc37ef813977c1e6c7bdc94f6a8686c321a67b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=1c76b8f75cfc13325be1634bfcd80ca5696587efd6c86a17cd833d0d30a1c982\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built segtok mpld3 sqlitedict langdetect sacremoses\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: segtok, deprecated, pluggy, pytest, sacremoses, tokenizers, sentencepiece, transformers, mpld3, bpemb, sqlitedict, langdetect, flair\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed bpemb-0.3.0 deprecated-1.2.10 flair-0.4.5 langdetect-1.0.8 mpld3-0.3 pluggy-0.13.1 pytest-5.4.2 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.91 sqlitedict-1.6.0 tokenizers-0.7.0 transformers-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FPeSFHHvWwwq",
        "outputId": "d825c925-23ba-4b42-ec8a-afbb9e5625d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import json\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "import flair\n",
        "import torch\n",
        "torch.__version__ = '1.5.0'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "75MkyG4iA9n9",
        "outputId": "5a1e0fb0-5ecb-4dd5-b1d0-c899864968c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "vader = SentimentIntensityAnalyzer()\n",
        "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-23 03:28:35,910 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/classy-imdb-en-rnn-cuda%3A0/imdb-v0.4.pt not found in cache, downloading to /tmp/tmpffywq47n\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501979561/1501979561 [01:02<00:00, 24036028.44B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-23 03:29:38,954 copying /tmp/tmpffywq47n to cache at /root/.flair/models/imdb-v0.4.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-23 03:29:45,213 removing temp file /tmp/tmpffywq47n\n",
            "2020-05-23 03:29:45,347 loading file /root/.flair/models/imdb-v0.4.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pT7Hdi2lW1NR",
        "outputId": "472cd117-dcb2-42f3-f149-71cc4c9de822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data = json.load(open(\"Data/fixed_valid.json\"))\n",
        "print(len(train_data.keys()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qR81njxAXc-_",
        "outputId": "babc93f3-826e-4e2c-a336-a7f2bce9d356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "results_data = {}\n",
        "train_data_model1, train_data_model2, train_data_model3, train_data_model4 = [], [], [], []\n",
        "for i,key in enumerate(train_data.keys()):\n",
        "    speaker = train_data[key]['dialog'][0]['speaker']\n",
        "                             \n",
        "    train_data_model1_string = str(train_data[key][\"prompt\"]).replace(\"|\",\"\")\n",
        "    train_data_model2_string = str(train_data[key][\"prompt\"]).replace(\"|\",\"\") + \" \" + str(train_data[key]['dialog'][0]['utterance']).replace(\"|\",\"\")\n",
        "    train_data_model3_string = str(train_data[key][\"prompt\"]).replace(\"|\",\"\")\n",
        "    train_data_model4_string = str(train_data[key][\"prompt\"]).replace(\"|\",\"\")\n",
        "    \n",
        "    for item in train_data[key]['dialog']:\n",
        "        train_data_model4_string += \" \" + str(item['utterance']).replace(\"|\",\"\")\n",
        "        if item['speaker'] == speaker:\n",
        "            train_data_model3_string += \" \" + str(item['utterance']).replace(\"|\",\"\")\n",
        "    \n",
        "    s1 = flair.data.Sentence(train_data_model1_string)\n",
        "    s2 = flair.data.Sentence(train_data_model2_string)\n",
        "    s3 = flair.data.Sentence(train_data_model3_string)\n",
        "    s4 = flair.data.Sentence(train_data_model4_string)\n",
        "    \n",
        "    flair_sentiment.predict(s1)\n",
        "    flair_sentiment.predict(s2)\n",
        "    flair_sentiment.predict(s3)\n",
        "    flair_sentiment.predict(s4)\n",
        "    \n",
        "    \n",
        "    results_data[key] = {\"emotion\": train_data[key][\"emotion\"], \"results\": {\n",
        "      \"model1\" : {\"input\": train_data_model1_string, \"vader\": vader.polarity_scores(train_data_model1_string), \n",
        "                  \"textblob\": TextBlob(train_data_model1_string).sentiment.polarity, \"flair\":s1.labels[0].to_dict()}, \n",
        "      \"model2\" : {\"input\": train_data_model2_string, \"vader\": vader.polarity_scores(train_data_model2_string), \n",
        "                  \"textblob\": TextBlob(train_data_model2_string).sentiment.polarity, \"flair\":s2.labels[0].to_dict()}, \n",
        "      \"model3\" : {\"input\": train_data_model3_string, \"vader\": vader.polarity_scores(train_data_model3_string), \n",
        "                  \"textblob\": TextBlob(train_data_model3_string).sentiment.polarity, \"flair\":s3.labels[0].to_dict()}, \n",
        "      \"model4\" : {\"input\": train_data_model4_string, \"vader\": vader.polarity_scores(train_data_model4_string), \n",
        "                  \"textblob\": TextBlob(train_data_model4_string).sentiment.polarity, \"flair\":s4.labels[0].to_dict()}\n",
        "      }}\n",
        "    \n",
        "    #emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\n",
        "\n",
        "    line_array_model1, line_array_model2, line_array_model3, line_array_model4 = [], [], [], []\n",
        "\n",
        "    line_array_model1.append(train_data[key][\"emotion\"])\n",
        "    if (train_data[key][\"emotion\"]).lower() in [\"disgusted\",\"devastated\", \"terrified\", \"anxious\", \"furious\", \"embarrassed\",\n",
        "                      \"angry\", \"sad\", \"afraid\", \"annoyed\", \"jealous\", \"ashamed\", \"lonely\", \"guilty\",\n",
        "                      \"apprehensive\", \"disappointed\"]:\n",
        "        line_array_model1.append(\"negative\")\n",
        "    else:\n",
        "        line_array_model1.append(\"positive\")\n",
        "    line_array_model1.append(train_data_model1_string)\n",
        "    line_array_model1.append(str(vader.polarity_scores(train_data_model1_string)[\"neg\"]))\n",
        "    line_array_model1.append(str(vader.polarity_scores(train_data_model1_string)[\"neu\"]))\n",
        "    line_array_model1.append(str(vader.polarity_scores(train_data_model1_string)[\"pos\"]))\n",
        "    line_array_model1.append(str(vader.polarity_scores(train_data_model1_string)[\"compound\"]))\n",
        "    line_array_model1.append(str(TextBlob(train_data_model1_string).sentiment.polarity))\n",
        "    line_array_model1.append(str(s1.labels[0].to_dict()[\"value\"]))\n",
        "    line_array_model1.append(str(s1.labels[0].to_dict()[\"confidence\"]))\n",
        "                        \n",
        "    line_array_model2.append(train_data[key][\"emotion\"])\n",
        "    if (train_data[key][\"emotion\"]).lower() in [\"disgusted\",\"devastated\", \"terrified\", \"anxious\", \"furious\", \"embarrassed\",\n",
        "                      \"angry\", \"sad\", \"afraid\", \"annoyed\", \"jealous\", \"ashamed\", \"lonely\", \"guilty\",\n",
        "                      \"apprehensive\", \"disappointed\"]:\n",
        "        line_array_model2.append(\"negative\")\n",
        "    else:\n",
        "        line_array_model2.append(\"positive\")\n",
        "    line_array_model2.append(train_data_model2_string)\n",
        "    line_array_model2.append(str(vader.polarity_scores(train_data_model2_string)[\"neg\"]))\n",
        "    line_array_model2.append(str(vader.polarity_scores(train_data_model2_string)[\"neu\"]))\n",
        "    line_array_model2.append(str(vader.polarity_scores(train_data_model2_string)[\"pos\"]))\n",
        "    line_array_model2.append(str(vader.polarity_scores(train_data_model2_string)[\"compound\"]))\n",
        "    line_array_model2.append(str(TextBlob(train_data_model2_string).sentiment.polarity))\n",
        "    line_array_model2.append(str(s2.labels[0].to_dict()[\"value\"]))\n",
        "    line_array_model2.append(str(s2.labels[0].to_dict()[\"confidence\"]))\n",
        "\n",
        "    line_array_model3.append(train_data[key][\"emotion\"])\n",
        "    if (train_data[key][\"emotion\"]).lower() in [\"disgusted\",\"devastated\", \"terrified\", \"anxious\", \"furious\", \"embarrassed\",\n",
        "                      \"angry\", \"sad\", \"afraid\", \"annoyed\", \"jealous\", \"ashamed\", \"lonely\", \"guilty\",\n",
        "                      \"apprehensive\", \"disappointed\"]:\n",
        "        line_array_model3.append(\"negative\")\n",
        "    else:\n",
        "        line_array_model3.append(\"positive\")\n",
        "    line_array_model3.append(train_data_model3_string.replace(\"|\",\"\"))\n",
        "    line_array_model3.append(str(vader.polarity_scores(train_data_model3_string)[\"neg\"]))\n",
        "    line_array_model3.append(str(vader.polarity_scores(train_data_model3_string)[\"neu\"]))\n",
        "    line_array_model3.append(str(vader.polarity_scores(train_data_model3_string)[\"pos\"]))\n",
        "    line_array_model3.append(str(vader.polarity_scores(train_data_model3_string)[\"compound\"]))\n",
        "    line_array_model3.append(str(TextBlob(train_data_model3_string).sentiment.polarity))\n",
        "    line_array_model3.append(str(s3.labels[0].to_dict()[\"value\"]))\n",
        "    line_array_model3.append(str(s3.labels[0].to_dict()[\"confidence\"]))\n",
        "                        \n",
        "    line_array_model4.append(train_data[key][\"emotion\"])\n",
        "    if (train_data[key][\"emotion\"]).lower() in [\"disgusted\",\"devastated\", \"terrified\", \"anxious\", \"furious\", \"embarrassed\",\n",
        "                      \"angry\", \"sad\", \"afraid\", \"annoyed\", \"jealous\", \"ashamed\", \"lonely\", \"guilty\",\n",
        "                      \"apprehensive\", \"disappointed\"]:\n",
        "        line_array_model4.append(\"negative\")\n",
        "    else:\n",
        "        line_array_model4.append(\"positive\")\n",
        "    line_array_model4.append(train_data_model4_string)\n",
        "    line_array_model4.append(str(vader.polarity_scores(train_data_model4_string)[\"neg\"]))\n",
        "    line_array_model4.append(str(vader.polarity_scores(train_data_model4_string)[\"neu\"]))\n",
        "    line_array_model4.append(str(vader.polarity_scores(train_data_model4_string)[\"pos\"]))\n",
        "    line_array_model4.append(str(vader.polarity_scores(train_data_model4_string)[\"compound\"]))\n",
        "    line_array_model4.append(str(TextBlob(train_data_model4_string).sentiment.polarity))\n",
        "    line_array_model4.append(str(s4.labels[0].to_dict()[\"value\"]))\n",
        "    line_array_model4.append(str(s4.labels[0].to_dict()[\"confidence\"]))\n",
        "\n",
        "    train_data_model1.append(\"|\".join(line_array_model1))\n",
        "    train_data_model2.append(\"|\".join(line_array_model2))\n",
        "    train_data_model3.append(\"|\".join(line_array_model3))\n",
        "    train_data_model4.append(\"|\".join(line_array_model4))\n",
        "\n",
        "    if i%1000 == 999:\n",
        "      print(i)\n",
        "      '''\n",
        "      filename = \"Data/data_sample_fixed_processed_\" + str(i) + \".json\"\n",
        "      with open(filename, 'w') as outfile:\n",
        "          outfile.write(json.dumps(results_data, indent=4))\n",
        "          results_data = {}\n",
        "\n",
        "      filename = \"Data/data_sample_fixed_processed_\" + \"model1_\" + str(i) + \".csv\"\n",
        "      with open(filename, 'w') as outfile:\n",
        "          outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "          outfile.write(\"\\n\".join(train_data_model1))\n",
        "                            \n",
        "      filename = \"Data/data_sample_fixed_processed_\" + \"model2_\" + str(i) + \".csv\"\n",
        "      with open(filename, 'w') as outfile:\n",
        "          outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "          outfile.write(\"\\n\".join(train_data_model2))\n",
        "                                  \n",
        "      filename = \"Data/data_sample_fixed_processed_\" + \"model3_\" + str(i) + \".csv\"\n",
        "      with open(filename, 'w') as outfile:\n",
        "          outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "          for line in train_data_model3:\n",
        "              try:\n",
        "                  outfile.write(line)\n",
        "                  outfile.write(\"\\n\")\n",
        "              except UnicodeEncodeError:\n",
        "                  print(line)\n",
        "                                  \n",
        "      filename = \"Data/data_sample_fixed_processed_\" + \"model4_\" + str(i) + \".csv\"\n",
        "      with open(filename, 'w') as outfile:\n",
        "          outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "          for line in train_data_model4:\n",
        "              try:\n",
        "                  outfile.write(line)\n",
        "                  outfile.write(\"\\n\")\n",
        "              except UnicodeEncodeError:\n",
        "                  print(line)\n",
        "    '''    \n",
        "filename = \"Data/valid_fixed_processed_final.json\"\n",
        "with open(filename, 'w') as outfile:\n",
        "    outfile.write(json.dumps(results_data, indent=4))\n",
        "    results_data = {}\n",
        "filename = \"Data/valid_fixed_processed_\" + \"model1_\" + \"final.csv\"\n",
        "with open(filename, 'w') as outfile:\n",
        "    outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "    outfile.write(\"\\n\".join(train_data_model1))\n",
        "                            \n",
        "filename = \"Data/valid_fixed_processed_\" + \"model2_\" + \"final.csv\"\n",
        "with open(filename, 'w') as outfile:\n",
        "    outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "    outfile.write(\"\\n\".join(train_data_model2))\n",
        "                           \n",
        "filename = \"Data/valid_fixed_processed_\" + \"model3_\" + \"final.csv\"\n",
        "with open(filename, 'w') as outfile:\n",
        "    outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "    for line in train_data_model3:\n",
        "        try:\n",
        "            outfile.write(line)\n",
        "            outfile.write(\"\\n\")\n",
        "        except UnicodeEncodeError:\n",
        "            print(line)\n",
        "                            \n",
        "filename = \"Data/valid_fixed_processed_\" + \"model4_\" + \"final.csv\"\n",
        "with open(filename, 'w') as outfile:\n",
        "    outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "    for line in train_data_model4:\n",
        "        try:\n",
        "            outfile.write(line)\n",
        "            outfile.write(\"\\n\")\n",
        "        except UnicodeEncodeError:\n",
        "            print(line)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999\n",
            "1999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mZTAnlOycwTs",
        "outputId": "7eeacf75-e183-4840-dcd4-9f7209694ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(train_data_model1)\n",
        "#print(train_data_model2[0])\n",
        "#print(train_data_model3[0])\n",
        "#print(train_data_model4[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0.127', '0.722', '0.151', '0.1119', '-0.20357142857142857', 'sad|sad|We (family friends) planned a holiday trip to go for the last weekend. I was preparing for that like packing dress and getting ready to it. Unfortunately one of my cousin was unable to come due to his personal reasons. So we just cancelled the trip. I was feeling sad for this trip cancellation. ', '0.0', '0.72', '0.28', '0.5413', '0.25', 'jealous|jealous|My cousin bought a really pretty shirt that I had been wanting.', '0.228', '0.685', '0.088', '-0.6256', '-0.012500000000000011', \"jealous|jealous|I was at the store and I tried on a dress_comma_ and it didn't look good. I saw someone else wearing it_comma_ looking great_comma_ and it made me feel pretty insecure and sad. ]\", '0.0', '1.0', '0.0', '0.0', '0.0', 'caring|caring|I was outside walking one day. And then I found a kitten on the road.', '0.07', '0.553', '0.377', '0.7964', '0.18958333333333333', 'proud|proud|I won first place in the marathon last weekend. I trained hard and deserved the win', '0.0', '1.0', '0.0', '0.0', '0.0', 'ashamed|ashamed|I stole a candy and was called out on it.', '0.4', '0.6', '0.0', '-0.25', '0.0', 'lonely|lonely|i felt all alone yesterday', '0.149', '0.721', '0.13', '-0.1027', '-0.075', 'furious|furious|When I got my test back and my professor had marked everything wrong. I clearly had the correct answers.', '0.0', '1.0', '0.0', '0.0', '0.0', 'excited|excited|I was getting my nose pierced tomorrow!', '0.0', '1.0', '0.0', '0.0', '0.0', 'anxious|anxious|im waiting for bloodwork to come back']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DQy3oR7ejcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(json.dumps(results_data['1022'], indent=4))\n",
        "with open('Data/data_sample_100_processed.json', 'w') as outfile:\n",
        "    outfile.write(json.dumps(results_data, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hwMRjRdejcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence|model_value|model_confidence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8nZRxKiejcQ",
        "colab_type": "code",
        "outputId": "7ec52ad7-82ba-4ec7-ec0b-44d75c61ba72",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}