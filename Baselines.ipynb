{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baselines.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkarpat/EmpatheticDialoguesEmotionDetection/blob/master/Baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ed6RYn3sV9d0",
        "outputId": "0124900c-4902-488e-a096-17da13ca40fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Colab settings/mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My\\ Drive/CSE\\ 245\\ Project"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/CSE 245 Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ybznvtw9WnuJ",
        "outputId": "f5b22eb3-75cf-4e66-8dfc-b0f0d1857dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "!ls Data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Data Exploration.ipynb'\n",
            " data_fixed_train.json\n",
            " data_sample_100.json\n",
            " data_sample_10.json\n",
            " data_sample_10_processed.json\n",
            " data_sample_fixed_processed_0.json\n",
            " data_sample_fixed_processed_10000.json\n",
            " data_sample_fixed_processed_1000.json\n",
            " data_sample_fixed_processed_11000.json\n",
            " data_sample_fixed_processed_12000.json\n",
            " data_sample_fixed_processed_13000.json\n",
            " data_sample_fixed_processed_2000.json\n",
            " data_sample_fixed_processed_3000.json\n",
            " data_sample_fixed_processed_4000.json\n",
            " data_sample_fixed_processed_5000.json\n",
            " data_sample_fixed_processed_6000.json\n",
            " data_sample_fixed_processed_7000.json\n",
            " data_sample_fixed_processed_8000.json\n",
            " data_sample_fixed_processed_9000.json\n",
            " data_sample_fixed_processed_model1_0.csv\n",
            " data_sample_fixed_processed_model1.csv\n",
            " data_sample_fixed_processed_model2_0.csv\n",
            " data_sample_fixed_processed_model2.csv\n",
            " data_sample_fixed_processed_model3_0.csv\n",
            " data_sample_fixed_processed_model3.csv\n",
            " data_sample_fixed_processed_model4_0.csv\n",
            " data_sample_fixed_processed_model4.csv\n",
            " fixed_train_516.csv\n",
            "'informative words.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G-95D-Lm_PUt",
        "outputId": "4419d8a4-59fa-46ca-ac60-c1ff33f35f6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "!pip install textblob\n",
        "!pip install flair"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.12.0)\n",
            "Collecting flair\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/29/81e3c9a829ec50857c23d82560941625f6b42ce76ee7c56ea9529e959d18/flair-0.4.5-py3-none-any.whl (136kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 7.6MB/s \n",
            "\u001b[?25hCollecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/f9/9f2b6c672c8f8bb87a4c1bd52c1b57213627b035305aad745d015b2a62ae/pytest-5.4.2-py3-none-any.whl (247kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 8.7MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Collecting bpemb>=0.2.9\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.0+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 15.8MB/s \n",
            "\u001b[?25hCollecting transformers>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 30.9MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.1.9)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.8.1)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.6.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (19.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.3)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (8.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (1.18.4)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/88/49e772d686088e1278766ad68a463513642a2a877487decbd691dec02955/sentencepiece-0.1.90-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 57.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (0.16.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->flair) (1.12.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 57.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 56.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.3.0->flair) (0.7)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.4)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.14.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.3.0->flair) (7.1.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (1.13.4)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (1.16.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.4->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair) (0.15.2)\n",
            "Building wheels for collected packages: segtok, langdetect, sqlitedict, mpld3, sacremoses\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25020 sha256=fba7c4b88be4bbd15e47e9127825a6edbe3d2ea3bd46c59c07e1806bcd4bff1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=a392889735f58bdeda5b132afa9427e875cf245978f45dc4fb43b7a7a39ce335\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.6.0-cp36-none-any.whl size=14689 sha256=1e7b5ff024c3ae578c818b7ce38842d88c3a0985196422539cd14952772b9304\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116679 sha256=4e39454a8a610fcc6ed4eba8e1aaf97a4fd3eb130a434bec58fcdbccb72a8a00\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c66663ad1e7f459cfa2a19ef257ef426d02150c6329e0350a0247adcb12d02c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built segtok langdetect sqlitedict mpld3 sacremoses\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pluggy, pytest, segtok, sentencepiece, bpemb, langdetect, tokenizers, sacremoses, transformers, sqlitedict, deprecated, mpld3, flair\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed bpemb-0.3.0 deprecated-1.2.10 flair-0.4.5 langdetect-1.0.8 mpld3-0.3 pluggy-0.13.1 pytest-5.4.2 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.90 sqlitedict-1.6.0 tokenizers-0.7.0 transformers-2.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FPeSFHHvWwwq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "075a7b3c-1eb1-466e-c62b-0604ab136a09"
      },
      "source": [
        "import json\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "import flair\n",
        "import torch\n",
        "torch.__version__ = '1.5.0'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "75MkyG4iA9n9",
        "outputId": "740f979f-f80c-431b-9787-3e1ca3d8306c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "vader = SentimentIntensityAnalyzer()\n",
        "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-19 15:10:01,034 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.4/classy-imdb-en-rnn-cuda%3A0/imdb-v0.4.pt not found in cache, downloading to /tmp/tmpnuh960gu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501979561/1501979561 [00:20<00:00, 74820937.80B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-19 15:10:21,252 copying /tmp/tmpnuh960gu to cache at /root/.flair/models/imdb-v0.4.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-05-19 15:10:30,609 removing temp file /tmp/tmpnuh960gu\n",
            "2020-05-19 15:10:30,752 loading file /root/.flair/models/imdb-v0.4.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pT7Hdi2lW1NR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "977009a3-c7ce-400a-ce2b-09ecfbe79f16"
      },
      "source": [
        "train_data = json.load(open(\"Data/data_fixed_train.json\"))\n",
        "print(len(train_data.keys()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qR81njxAXc-_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "836a6e08-4836-4a5c-b85f-eb6bd61c67dc"
      },
      "source": [
        "results_data = {}\n",
        "train_data_model1, train_data_model2, train_data_model3, train_data_model4 = [], [], [], []\n",
        "for i,key in enumerate(train_data.keys()):\n",
        "    '''\n",
        "    if i < 9001:\n",
        "      continue\n",
        "    '''\n",
        "    speaker = train_data[key]['dialog'][0]['speaker']\n",
        "                             \n",
        "    train_data_model1_string = str(train_data[key][\"prompt\"])\n",
        "    train_data_model2_string = str(train_data[key][\"prompt\"]) + \" \" + str(train_data[key]['dialog'][0]['utterance'])\n",
        "    train_data_model3_string = str(train_data[key][\"prompt\"])\n",
        "    train_data_model4_string = str(train_data[key][\"prompt\"])\n",
        "    \n",
        "    for item in train_data[key]['dialog']:\n",
        "        train_data_model4_string += \" \" + str(item['utterance'])\n",
        "        if item['speaker'] == speaker:\n",
        "            train_data_model3_string += \" \" + str(item['utterance'])\n",
        "                             \n",
        "    #train_data_model1.append(train_data_model1_string)\n",
        "    #train_data_model2.append(train_data_model2_string)\n",
        "    #train_data_model3.append(train_data_model3_string)\n",
        "    #train_data_model4.append(train_data_model4_string)\n",
        "    \n",
        "    s1 = flair.data.Sentence(train_data_model1_string)\n",
        "    '''\n",
        "    s2 = flair.data.Sentence(train_data_model2_string)\n",
        "    s3 = flair.data.Sentence(train_data_model3_string)\n",
        "    s4 = flair.data.Sentence(train_data_model4_string)\n",
        "    '''\n",
        "    \n",
        "    flair_sentiment.predict(s1)\n",
        "    '''\n",
        "    flair_sentiment.predict(s2)\n",
        "    flair_sentiment.predict(s3)\n",
        "    flair_sentiment.predict(s4)\n",
        "    \n",
        "    \n",
        "    results_data[key] = {\"emotion\": train_data[key][\"emotion\"], \"results\": {\n",
        "      \"model1\" : {\"input\": train_data_model1_string, \"vader\": vader.polarity_scores(train_data_model1_string), \n",
        "                  \"textblob\": TextBlob(train_data_model1_string).sentiment.polarity, \"flair\":s1.labels[0].to_dict()}, \n",
        "      \"model2\" : {\"input\": train_data_model2_string, \"vader\": vader.polarity_scores(train_data_model2_string), \n",
        "                  \"textblob\": TextBlob(train_data_model2_string).sentiment.polarity, \"flair\":s2.labels[0].to_dict()}, \n",
        "      \"model3\" : {\"input\": train_data_model3_string, \"vader\": vader.polarity_scores(train_data_model3_string), \n",
        "                  \"textblob\": TextBlob(train_data_model3_string).sentiment.polarity, \"flair\":s3.labels[0].to_dict()}, \n",
        "      \"model4\" : {\"input\": train_data_model4_string, \"vader\": vader.polarity_scores(train_data_model4_string), \n",
        "                  \"textblob\": TextBlob(train_data_model4_string).sentiment.polarity, \"flair\":s4.labels[0].to_dict()}\n",
        "      }}\n",
        "    \n",
        "    #emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\n",
        "    '''\n",
        "    line_array_model1, line_array_model2, line_array_model3, line_array_model4 = [], [], [], []\n",
        "    \n",
        "    line_array_model1.append(train_data[key][\"emotion\"])\n",
        "    if (train_data[key][\"emotion\"]).lower() in [\"disgusted\",\"devastated\", \"terrified\", \"anxious\", \"furious\", \"embarrassed\",\n",
        "                      \"angry\", \"sad\", \"afraid\", \"annoyed\", \"jealous\", \"ashamed\", \"lonely\", \"guilty\",\n",
        "                      \"apprehensive\", \"disappointed\"]:\n",
        "        line_array_model1.append(\"negative\")\n",
        "    else:\n",
        "        line_array_model1.append(\"positive\")\n",
        "    line_array_model1.append(train_data_model1_string)\n",
        "    line_array_model1.append(str(vader.polarity_scores(train_data_model1_string)[\"neg\"]))\n",
        "    line_array_model1.append(str(vader.polarity_scores(train_data_model1_string)[\"neu\"]))\n",
        "    line_array_model1.append(str(vader.polarity_scores(train_data_model1_string)[\"pos\"]))\n",
        "    line_array_model1.append(str(vader.polarity_scores(train_data_model1_string)[\"compound\"]))\n",
        "    line_array_model1.append(str(TextBlob(train_data_model1_string).sentiment.polarity))\n",
        "    line_array_model1.append(str(s1.labels[0].to_dict()[\"value\"]))\n",
        "    line_array_model1.append(str(s1.labels[0].to_dict()[\"confidence\"]))\n",
        "\n",
        "    '''                         \n",
        "    line_array_model2.append(train_data[key][\"emotion\"])\n",
        "    if (train_data[key][\"emotion\"]).lower() in [\"disgusted\",\"devastated\", \"terrified\", \"anxious\", \"furious\", \"embarrassed\",\n",
        "                      \"angry\", \"sad\", \"afraid\", \"annoyed\", \"jealous\", \"ashamed\", \"lonely\", \"guilty\",\n",
        "                      \"apprehensive\", \"disappointed\"]:\n",
        "        line_array_model2.append(\"negative\")\n",
        "    else:\n",
        "        line_array_model2.append(\"positive\")\n",
        "    line_array_model2.append(train_data_model2_string)\n",
        "    line_array_model2.append(str(vader.polarity_scores(train_data_model2_string)[\"neg\"]))\n",
        "    line_array_model2.append(str(vader.polarity_scores(train_data_model2_string)[\"neu\"]))\n",
        "    line_array_model2.append(str(vader.polarity_scores(train_data_model2_string)[\"pos\"]))\n",
        "    line_array_model2.append(str(vader.polarity_scores(train_data_model2_string)[\"compound\"]))\n",
        "    line_array_model2.append(str(TextBlob(train_data_model2_string).sentiment.polarity))\n",
        "    line_array_model2.append(str(s2.labels[0].to_dict()[\"value\"]))\n",
        "    line_array_model2.append(str(s2.labels[0].to_dict()[\"confidence\"]))\n",
        "\n",
        "    line_array_model3.append(train_data[key][\"emotion\"])\n",
        "    if (train_data[key][\"emotion\"]).lower() in [\"disgusted\",\"devastated\", \"terrified\", \"anxious\", \"furious\", \"embarrassed\",\n",
        "                      \"angry\", \"sad\", \"afraid\", \"annoyed\", \"jealous\", \"ashamed\", \"lonely\", \"guilty\",\n",
        "                      \"apprehensive\", \"disappointed\"]:\n",
        "        line_array_model3.append(\"negative\")\n",
        "    else:\n",
        "        line_array_model3.append(\"positive\")\n",
        "    line_array_model3.append(train_data_model3_string)\n",
        "    line_array_model3.append(str(vader.polarity_scores(train_data_model3_string)[\"neg\"]))\n",
        "    line_array_model3.append(str(vader.polarity_scores(train_data_model3_string)[\"neu\"]))\n",
        "    line_array_model3.append(str(vader.polarity_scores(train_data_model3_string)[\"pos\"]))\n",
        "    line_array_model3.append(str(vader.polarity_scores(train_data_model3_string)[\"compound\"]))\n",
        "    line_array_model3.append(str(TextBlob(train_data_model3_string).sentiment.polarity))\n",
        "    line_array_model3.append(str(s3.labels[0].to_dict()[\"value\"]))\n",
        "    line_array_model3.append(str(s3.labels[0].to_dict()[\"confidence\"]))\n",
        "                             \n",
        "    line_array_model4.append(train_data[key][\"emotion\"])\n",
        "    if (train_data[key][\"emotion\"]).lower() in [\"disgusted\",\"devastated\", \"terrified\", \"anxious\", \"furious\", \"embarrassed\",\n",
        "                      \"angry\", \"sad\", \"afraid\", \"annoyed\", \"jealous\", \"ashamed\", \"lonely\", \"guilty\",\n",
        "                      \"apprehensive\", \"disappointed\"]:\n",
        "        line_array_model4.append(\"negative\")\n",
        "    else:\n",
        "        line_array_model4.append(\"positive\")\n",
        "    line_array_model4.append(train_data_model4_string)\n",
        "    line_array_model4.append(str(vader.polarity_scores(train_data_model4_string)[\"neg\"]))\n",
        "    line_array_model4.append(str(vader.polarity_scores(train_data_model4_string)[\"neu\"]))\n",
        "    line_array_model4.append(str(vader.polarity_scores(train_data_model4_string)[\"pos\"]))\n",
        "    line_array_model4.append(str(vader.polarity_scores(train_data_model4_string)[\"compound\"]))\n",
        "    line_array_model4.append(str(TextBlob(train_data_model4_string).sentiment.polarity))\n",
        "    line_array_model4.append(str(s4.labels[0].to_dict()[\"value\"]))\n",
        "    line_array_model4.append(str(s4.labels[0].to_dict()[\"confidence\"]))\n",
        "    #print(line_array_model1)\n",
        "    #print(\"|\".join(line_array_model1))\n",
        "    '''\n",
        "    train_data_model1.append(\"|\".join(line_array_model1))\n",
        "    '''\n",
        "    train_data_model2.append(\"|\".join(line_array_model2))\n",
        "    train_data_model3.append(\"|\".join(line_array_model3))\n",
        "    train_data_model4.append(\"|\".join(line_array_model4))\n",
        "    \n",
        "    '''\n",
        "\n",
        "    if i%1000 == 999:\n",
        "      print(i)\n",
        "      '''\n",
        "      filename = \"Data/data_sample_fixed_processed_\" + str(i) + \".json\"\n",
        "      with open(filename, 'w') as outfile:\n",
        "          outfile.write(json.dumps(results_data, indent=4))\n",
        "          results_data = {}\n",
        "      '''\n",
        "      filename = \"Data/data_sample_fixed_processed_\" + \"model1_\" + str(i) + \".csv\"\n",
        "      with open(filename, 'w') as outfile:\n",
        "          outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "          outfile.write(\"\\n\".join(train_data_model1))\n",
        "      '''                            \n",
        "      filename = \"Data/data_sample_fixed_processed_\" + \"model2_\" + str(i) + \".csv\"\n",
        "      with open(filename, 'w') as outfile:\n",
        "          outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "          outfile.write(\"\\n\".join(train_data_model2))\n",
        "                                  \n",
        "      filename = \"Data/data_sample_fixed_processed_\" + \"model3_\" + str(i) + \".csv\"\n",
        "      with open(filename, 'w') as outfile:\n",
        "          outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "          for line in train_data_model3:\n",
        "              try:\n",
        "                  outfile.write(line)\n",
        "                  outfile.write(\"\\n\")\n",
        "              except UnicodeEncodeError:\n",
        "                  print(line)\n",
        "                                  \n",
        "      filename = \"Data/data_sample_fixed_processed_\" + \"model4_\" + str(i) + \".csv\"\n",
        "      with open(filename, 'w') as outfile:\n",
        "          outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "          for line in train_data_model4:\n",
        "              try:\n",
        "                  outfile.write(line)\n",
        "                  outfile.write(\"\\n\")\n",
        "              except UnicodeEncodeError:\n",
        "                  print(line)\n",
        "    '''\n",
        "filename = \"Data/data_sample_fixed_processed_final.json\"\n",
        "with open(filename, 'w') as outfile:\n",
        "    outfile.write(json.dumps(results_data, indent=4))\n",
        "    results_data = {}\n",
        "filename = \"Data/data_sample_fixed_processed_\" + \"model1_\" + \"final.csv\"\n",
        "with open(filename, 'w') as outfile:\n",
        "    outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "    outfile.write(\"\\n\".join(train_data_model1))\n",
        "'''                            \n",
        "filename = \"Data/data_sample_fixed_processed_\" + \"model2_\" + \"final.csv\"\n",
        "with open(filename, 'w') as outfile:\n",
        "    outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "    outfile.write(\"\\n\".join(train_data_model2))\n",
        "                            \n",
        "filename = \"Data/data_sample_fixed_processed_\" + \"model3_\" + \"final.csv\"\n",
        "with open(filename, 'w') as outfile:\n",
        "    outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "    for line in train_data_model3:\n",
        "        try:\n",
        "            outfile.write(line)\n",
        "            outfile.write(\"\\n\")\n",
        "        except UnicodeEncodeError:\n",
        "            print(line)\n",
        "                            \n",
        "filename = \"Data/data_sample_fixed_processed_\" + \"model4_\" + \"final.csv\"\n",
        "with open(filename, 'w') as outfile:\n",
        "    outfile.write(\"emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence\\n\")\n",
        "    for line in train_data_model4:\n",
        "        try:\n",
        "            outfile.write(line)\n",
        "            outfile.write(\"\\n\")\n",
        "        except UnicodeEncodeError:\n",
        "            print(line)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999\n",
            "1999\n",
            "2999\n",
            "3999\n",
            "4999\n",
            "5999\n",
            "6999\n",
            "7999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mZTAnlOycwTs",
        "outputId": "7eeacf75-e183-4840-dcd4-9f7209694ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(train_data_model1)\n",
        "#print(train_data_model2[0])\n",
        "#print(train_data_model3[0])\n",
        "#print(train_data_model4[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0.127', '0.722', '0.151', '0.1119', '-0.20357142857142857', 'sad|sad|We (family friends) planned a holiday trip to go for the last weekend. I was preparing for that like packing dress and getting ready to it. Unfortunately one of my cousin was unable to come due to his personal reasons. So we just cancelled the trip. I was feeling sad for this trip cancellation. ', '0.0', '0.72', '0.28', '0.5413', '0.25', 'jealous|jealous|My cousin bought a really pretty shirt that I had been wanting.', '0.228', '0.685', '0.088', '-0.6256', '-0.012500000000000011', \"jealous|jealous|I was at the store and I tried on a dress_comma_ and it didn't look good. I saw someone else wearing it_comma_ looking great_comma_ and it made me feel pretty insecure and sad. ]\", '0.0', '1.0', '0.0', '0.0', '0.0', 'caring|caring|I was outside walking one day. And then I found a kitten on the road.', '0.07', '0.553', '0.377', '0.7964', '0.18958333333333333', 'proud|proud|I won first place in the marathon last weekend. I trained hard and deserved the win', '0.0', '1.0', '0.0', '0.0', '0.0', 'ashamed|ashamed|I stole a candy and was called out on it.', '0.4', '0.6', '0.0', '-0.25', '0.0', 'lonely|lonely|i felt all alone yesterday', '0.149', '0.721', '0.13', '-0.1027', '-0.075', 'furious|furious|When I got my test back and my professor had marked everything wrong. I clearly had the correct answers.', '0.0', '1.0', '0.0', '0.0', '0.0', 'excited|excited|I was getting my nose pierced tomorrow!', '0.0', '1.0', '0.0', '0.0', '0.0', 'anxious|anxious|im waiting for bloodwork to come back']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DQy3oR7ejcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(json.dumps(results_data['1022'], indent=4))\n",
        "with open('Data/data_sample_100_processed.json', 'w') as outfile:\n",
        "    outfile.write(json.dumps(results_data, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hwMRjRdejcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emotion|processed_emotion|text|vader_neg|vader_neu|vader_pos|vader_compound|textblob|flair_value|flair_confidence|model_value|model_confidence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8nZRxKiejcQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "7ec52ad7-82ba-4ec7-ec0b-44d75c61ba72"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}