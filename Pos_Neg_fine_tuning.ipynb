{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pos-Neg fine tuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52b9f9bd42204a09ae84986706666e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3b31d13c02aa4319ac0ca0eab0827ec5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f30b3a1b0bf946f1aa18fdc2e3210455",
              "IPY_MODEL_5fab9cf346de4f98af1c872f6f8a06be"
            ]
          }
        },
        "3b31d13c02aa4319ac0ca0eab0827ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f30b3a1b0bf946f1aa18fdc2e3210455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_897bc61439714acc9cb7417ab41398ee",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03de98a8ffd945a2b564d7d33f961d39"
          }
        },
        "5fab9cf346de4f98af1c872f6f8a06be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f37c8aca65e54bfda5975022c970d25d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 635kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6416e0cee9844606b38f3dba7af8ce61"
          }
        },
        "897bc61439714acc9cb7417ab41398ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03de98a8ffd945a2b564d7d33f961d39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f37c8aca65e54bfda5975022c970d25d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6416e0cee9844606b38f3dba7af8ce61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ea931227cf6043bbb607f85a2583b622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8c4d5ac4ab5b4f29a31a53a635aa03b5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab3560b1ec9446b7a268109f6c93d81a",
              "IPY_MODEL_7fb0572ba9e14be6b780a5e66329700c"
            ]
          }
        },
        "8c4d5ac4ab5b4f29a31a53a635aa03b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab3560b1ec9446b7a268109f6c93d81a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_771b99c29fc54b5b957f2d0b65da4d76",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf67693e346a418dbe0ca05733d50ee9"
          }
        },
        "7fb0572ba9e14be6b780a5e66329700c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b842c13c9d6846f49e6aa07b18ea20bc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [01:02&lt;00:00, 6.91B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d28240c855b47d3984f85acabfa8b60"
          }
        },
        "771b99c29fc54b5b957f2d0b65da4d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf67693e346a418dbe0ca05733d50ee9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b842c13c9d6846f49e6aa07b18ea20bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d28240c855b47d3984f85acabfa8b60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9025d6b46ccd4804852c8beef34e80f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1a85bc1b249a491e9b14ac111b2798a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d2bebedf76cf4a779a45f93e7d3d8352",
              "IPY_MODEL_80f08eee879f4cd394982fc148ce65e0"
            ]
          }
        },
        "1a85bc1b249a491e9b14ac111b2798a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2bebedf76cf4a779a45f93e7d3d8352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce47afebe6e14d7a80dae5b78d067ba8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_966f7c81cc684ce8a62d307f79a38436"
          }
        },
        "80f08eee879f4cd394982fc148ce65e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ef7d0e0c16af426dbae5e278a8cba8ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:05&lt;00:00, 73.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a128b034857b407bbef8c5e80c67e797"
          }
        },
        "ce47afebe6e14d7a80dae5b78d067ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "966f7c81cc684ce8a62d307f79a38436": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef7d0e0c16af426dbae5e278a8cba8ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a128b034857b407bbef8c5e80c67e797": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkarpat/EmpatheticDialoguesEmotionDetection/blob/master/Pos_Neg_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BF5BmnrG4hb",
        "colab_type": "code",
        "outputId": "abf64662-5339-4f4f-9e0d-fc5facbf7710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Colab settings/mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My\\ Drive/CSE\\ 245\\ Project"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/CSE 245 Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJxrAKK0klhS",
        "colab_type": "code",
        "outputId": "13998697-4ffc-44ef-caed-ce5185766245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/97/7db72a0beef1825f82188a4b923e62a146271ac2ced7928baa4d47ef2467/transformers-2.9.1-py3-none-any.whl (641kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 22.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 58.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a86e080b53e1b3023eda9e1bc8c036135ddf3df8923d0a854f3ccb7ced193494\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2hf38gCHF5M",
        "colab_type": "code",
        "outputId": "c8b382c1-23bd-45e8-996d-d4c17b0b5e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "!ls Data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Data Exploration.ipynb'\n",
            " data_fixed_train.json\n",
            " data_sample_100.json\n",
            " data_sample_10.json\n",
            " data_sample_10_processed.json\n",
            " data_sample_fixed_processed_model1_final.csv\n",
            " data_sample_fixed_processed_model2_final.csv\n",
            " data_sample_fixed_processed_model3_final.csv\n",
            " data_sample_fixed_processed_model3_intermediate.csv\n",
            " data_sample_fixed_processed_model4_final.csv\n",
            " fixed\n",
            " fixed_test.json\n",
            " fixed_train_516.csv\n",
            " fixed_valid.json\n",
            "'informative words.ipynb'\n",
            " Raw\n",
            " test_fixed_processed_model1_final.csv\n",
            " test_fixed_processed_model2_final.csv\n",
            " test_fixed_processed_model3_final.csv\n",
            " test_fixed_processed_model4_final.csv\n",
            " valid.csv\n",
            " valid_fixed_processed_model1_final.csv\n",
            " valid_fixed_processed_model2_final.csv\n",
            " valid_fixed_processed_model3_final.csv\n",
            " valid_fixed_processed_model4_final.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvfjRg5VkTv2",
        "colab_type": "code",
        "outputId": "5fd327bd-94f6-43bb-e058-a7783d90e76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCtwwUXuHLyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf70qiYZHRr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv('Data/data_sample_fixed_processed_model1_final.csv', sep='|')\n",
        "valid_df = pd.read_csv('Data/valid_fixed_processed_model1_final.csv', sep='|')\n",
        "test_df = pd.read_csv('Data/test_fixed_processed_model1_final.csv', sep='|')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1180kF_JwFu",
        "colab_type": "code",
        "outputId": "d78fa476-08a8-4f8f-b68b-a05d4258d6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "print('Number of training sentences: {:,}\\n'.format(train_df.shape[0]))\n",
        "print('Number of valid sentences: {:,}\\n'.format(valid_df.shape[0]))\n",
        "print('Number of test sentences: {:,}\\n'.format(test_df.shape[0]))\n",
        "\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "train_df['label'] = train_df['processed_emotion'].apply(lambda x: 0 if x == \"negative\" else 1)\n",
        "valid_df['label'] = valid_df['processed_emotion'].apply(lambda x: 0 if x == \"negative\" else 1)\n",
        "test_df['label'] = test_df['processed_emotion'].apply(lambda x: 0 if x == \"negative\" else 1)\n",
        "train_df.sample(10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 19,533\n",
            "\n",
            "Number of valid sentences: 2,763\n",
            "\n",
            "Number of test sentences: 2,542\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>processed_emotion</th>\n",
              "      <th>text</th>\n",
              "      <th>vader_neg</th>\n",
              "      <th>vader_neu</th>\n",
              "      <th>vader_pos</th>\n",
              "      <th>vader_compound</th>\n",
              "      <th>textblob</th>\n",
              "      <th>flair_value</th>\n",
              "      <th>flair_confidence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17101</th>\n",
              "      <td>nostalgic</td>\n",
              "      <td>positive</td>\n",
              "      <td>Might sell my home. It's the home I grew up in.</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.961691</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1178</th>\n",
              "      <td>surprised</td>\n",
              "      <td>positive</td>\n",
              "      <td>just got out of work to my friend sending me a...</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.717</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0258</td>\n",
              "      <td>-0.400000</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.936500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8764</th>\n",
              "      <td>grateful</td>\n",
              "      <td>positive</td>\n",
              "      <td>I am so happy that I have healthy_comma_ happy...</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.551</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.3199</td>\n",
              "      <td>0.275000</td>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.896952</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12148</th>\n",
              "      <td>furious</td>\n",
              "      <td>negative</td>\n",
              "      <td>My 15 year old son took my ferrari out for a t...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.241667</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.995662</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2251</th>\n",
              "      <td>prepared</td>\n",
              "      <td>positive</td>\n",
              "      <td>i had worked with my pitchers all year. and it...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.857</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.3612</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.922604</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9816</th>\n",
              "      <td>nostalgic</td>\n",
              "      <td>positive</td>\n",
              "      <td>My mom sent me a box of stuff from my childhood.</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.879044</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11611</th>\n",
              "      <td>afraid</td>\n",
              "      <td>negative</td>\n",
              "      <td>I am so scared of my dog dying.</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.653</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.4927</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.859498</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4162</th>\n",
              "      <td>joyful</td>\n",
              "      <td>positive</td>\n",
              "      <td>I get to go to Italy in a few months! I've bee...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.250000</td>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.999917</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17004</th>\n",
              "      <td>guilty</td>\n",
              "      <td>negative</td>\n",
              "      <td>I felt bad when i noticed our dog went missing</td>\n",
              "      <td>0.487</td>\n",
              "      <td>0.513</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.6908</td>\n",
              "      <td>-0.450000</td>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.512273</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7333</th>\n",
              "      <td>sad</td>\n",
              "      <td>negative</td>\n",
              "      <td>Kind of bummed. I was supposed to meet my best...</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.660</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.2960</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.988691</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         emotion processed_emotion  ... flair_confidence  label\n",
              "17101  nostalgic          positive  ...         0.961691      1\n",
              "1178   surprised          positive  ...         0.936500      1\n",
              "8764    grateful          positive  ...         0.896952      1\n",
              "12148    furious          negative  ...         0.995662      0\n",
              "2251    prepared          positive  ...         0.922604      1\n",
              "9816   nostalgic          positive  ...         0.879044      1\n",
              "11611     afraid          negative  ...         0.859498      0\n",
              "4162      joyful          positive  ...         0.999917      1\n",
              "17004     guilty          negative  ...         0.512273      0\n",
              "7333         sad          negative  ...         0.988691      0\n",
              "\n",
              "[10 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru1WMhqmlWSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_train = train_df.text.values\n",
        "labels_train = train_df.label.values\n",
        "sentences_valid = valid_df.text.values\n",
        "labels_valid = valid_df.label.values\n",
        "sentences_test = test_df.text.values\n",
        "labels_test = test_df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjMYvwDwlgkQ",
        "colab_type": "code",
        "outputId": "f185bd4f-aea8-4ec5-d962-3d06a5a09cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "52b9f9bd42204a09ae84986706666e5d",
            "3b31d13c02aa4319ac0ca0eab0827ec5",
            "f30b3a1b0bf946f1aa18fdc2e3210455",
            "5fab9cf346de4f98af1c872f6f8a06be",
            "897bc61439714acc9cb7417ab41398ee",
            "03de98a8ffd945a2b564d7d33f961d39",
            "f37c8aca65e54bfda5975022c970d25d",
            "6416e0cee9844606b38f3dba7af8ce61"
          ]
        }
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52b9f9bd42204a09ae84986706666e5d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDApGWm2lmoy",
        "colab_type": "code",
        "outputId": "83519d60-5d55-49b0-99fe-b7f7fc506994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences_train[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences_train[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences_train[0])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.\n",
            "Tokenized:  ['i', 'remember', 'going', 'to', 'the', 'fireworks', 'with', 'my', 'best', 'friend', '.', 'there', 'was', 'a', 'lot', 'of', 'people', '_', 'com', '##ma', '_', 'but', 'it', 'only', 'felt', 'like', 'us', 'in', 'the', 'world', '.']\n",
            "Token IDs:  [1045, 3342, 2183, 2000, 1996, 16080, 2007, 2026, 2190, 2767, 1012, 2045, 2001, 1037, 2843, 1997, 2111, 1035, 4012, 2863, 1035, 2021, 2009, 2069, 2371, 2066, 2149, 1999, 1996, 2088, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32b9WeYpmSUH",
        "colab_type": "code",
        "outputId": "34f524c1-c0d9-4079-8e94-f556763e21ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "max_len = 0\n",
        "# For every sentence...\n",
        "for sent in sentences_train:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max train sentence length: ', max_len)\n",
        "\n",
        "max_len = 0\n",
        "# For every sentence...\n",
        "for sent in sentences_valid:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max valid sentence length: ', max_len)\n",
        "\n",
        "max_len = 0\n",
        "# For every sentence...\n",
        "for sent in sentences_test:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max test sentence length: ', max_len)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max train sentence length:  144\n",
            "Max valid sentence length:  170\n",
            "Max test sentence length:  139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hj4AKvVnRc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(sentences, labels):\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent in sentences:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length`\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = 256,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.    \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      \n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.from_numpy(labels)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', sentences[0])\n",
        "  print('Token IDs:', input_ids[0])\n",
        "  return(input_ids, attention_masks, labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odt6QrV8zNFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57b6bb1a-891f-4a16-89c6-ce8d8b85ad84"
      },
      "source": [
        "input_ids_train, attention_masks_train, labels_train = tokenize(sentences_train, labels_train)\n",
        "input_ids_valid, attention_masks_valid, labels_valid = tokenize(sentences_valid, labels_valid)\n",
        "input_ids_test, attention_masks_test, labels_test = tokenize(sentences_test, labels_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  I remember going to the fireworks with my best friend. There was a lot of people_comma_ but it only felt like us in the world.\n",
            "Token IDs: tensor([  101,  1045,  3342,  2183,  2000,  1996, 16080,  2007,  2026,  2190,\n",
            "         2767,  1012,  2045,  2001,  1037,  2843,  1997,  2111,  1035,  4012,\n",
            "         2863,  1035,  2021,  2009,  2069,  2371,  2066,  2149,  1999,  1996,\n",
            "         2088,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0])\n",
            "Original:  I've been at my current job 5 years. I'm at the point I can go into upper management but I'm afraid. Maybe I'm just comfortable now. I don't know. \n",
            "Token IDs: tensor([ 101, 1045, 1005, 2310, 2042, 2012, 2026, 2783, 3105, 1019, 2086, 1012,\n",
            "        1045, 1005, 1049, 2012, 1996, 2391, 1045, 2064, 2175, 2046, 3356, 2968,\n",
            "        2021, 1045, 1005, 1049, 4452, 1012, 2672, 1045, 1005, 1049, 2074, 6625,\n",
            "        2085, 1012, 1045, 2123, 1005, 1056, 2113, 1012,  102,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0])\n",
            "Original:  I felt guilty when I was driving home one night and a person tried to fly into my lane_comma_ and didn't see me. I honked and they swerved back into their lane_comma_ slammed on their brakes_comma_ and hit the water cones.\n",
            "Token IDs: tensor([  101,  1045,  2371,  5905,  2043,  1045,  2001,  4439,  2188,  2028,\n",
            "         2305,  1998,  1037,  2711,  2699,  2000,  4875,  2046,  2026,  4644,\n",
            "         1035,  4012,  2863,  1035,  1998,  2134,  1005,  1056,  2156,  2033,\n",
            "         1012,  1045, 10189,  8126,  1998,  2027, 25430, 25944,  2067,  2046,\n",
            "         2037,  4644,  1035,  4012,  2863,  1035,  7549,  2006,  2037, 13627,\n",
            "         1035,  4012,  2863,  1035,  1998,  2718,  1996,  2300, 23825,  1012,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY-7JF7uoB4n",
        "colab_type": "code",
        "outputId": "50473142-9799-4cb8-a403-db2f256783d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
        "dataset_valid = TensorDataset(input_ids_valid, attention_masks_valid, labels_valid)\n",
        "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset_train))\n",
        "val_size = len(dataset_train) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "#train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "#Use the given training and validation datasets\n",
        "train_dataset, val_dataset = dataset_train, dataset_valid\n",
        "\n",
        "print('{:>5,} training samples'.format(len(train_dataset)))\n",
        "print('{:>5,} validation samples'.format(len(val_dataset)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19,533 training samples\n",
            "2,763 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcmIn2UMpl9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyDoYRF9prgX",
        "colab_type": "code",
        "outputId": "0e102317-3e57-4cde-c4b4-e00598bc787e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ea931227cf6043bbb607f85a2583b622",
            "8c4d5ac4ab5b4f29a31a53a635aa03b5",
            "ab3560b1ec9446b7a268109f6c93d81a",
            "7fb0572ba9e14be6b780a5e66329700c",
            "771b99c29fc54b5b957f2d0b65da4d76",
            "cf67693e346a418dbe0ca05733d50ee9",
            "b842c13c9d6846f49e6aa07b18ea20bc",
            "1d28240c855b47d3984f85acabfa8b60",
            "9025d6b46ccd4804852c8beef34e80f9",
            "1a85bc1b249a491e9b14ac111b2798a6",
            "d2bebedf76cf4a779a45f93e7d3d8352",
            "80f08eee879f4cd394982fc148ce65e0",
            "ce47afebe6e14d7a80dae5b78d067ba8",
            "966f7c81cc684ce8a62d307f79a38436",
            "ef7d0e0c16af426dbae5e278a8cba8ff",
            "a128b034857b407bbef8c5e80c67e797"
          ]
        }
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea931227cf6043bbb607f85a2583b622",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9025d6b46ccd4804852c8beef34e80f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX5fg-Etp3NM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6lXVqvTqCLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 15\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsLX-3yRqZ8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evnX8oR8qawr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAODaR16qeOr",
        "colab_type": "code",
        "outputId": "0b5e5ab6-3d23-40a0-dac5-4f8bab480323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        loss, logits = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 15 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:46.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:31.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:24.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:16.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:09.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:02.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:54.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:47.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:39.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:32.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:25.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:17.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:10.\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:13:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.26\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 2 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:16.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:39.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:24.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:17.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epcoh took: 0:13:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.30\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 3 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:16.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:39.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:24.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:16.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:13:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 4 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:16.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:39.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:24.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:16.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epcoh took: 0:13:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.46\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 5 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:15.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:38.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:23.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:16.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:13:22\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.53\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 6 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:15.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:00.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:38.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:23.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:16.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:08.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:13:22\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 7 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:15.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:38.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:23.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:16.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:13:22\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 8 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:16.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:39.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:24.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:16.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:13:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.74\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 9 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:15.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:38.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:23.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:16.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:08.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:13:22\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.89\n",
            "  Validation Loss: 0.80\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 10 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:15.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:39.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:24.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:16.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:13:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.82\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 11 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:15.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:00.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:45.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:38.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:30.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:23.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:15.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:08.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:13:22\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.87\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 12 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:15.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:38.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:24.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:16.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:13:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.88\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 13 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:16.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:39.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:24.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:16.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:13:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.88\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 14 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:31.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:16.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:54.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:39.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:24.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:17.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:13:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.89\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "======== Epoch 15 / 15 ========\n",
            "Training...\n",
            "  Batch    40  of    611.    Elapsed: 0:00:53.\n",
            "  Batch    80  of    611.    Elapsed: 0:01:45.\n",
            "  Batch   120  of    611.    Elapsed: 0:02:38.\n",
            "  Batch   160  of    611.    Elapsed: 0:03:30.\n",
            "  Batch   200  of    611.    Elapsed: 0:04:23.\n",
            "  Batch   240  of    611.    Elapsed: 0:05:16.\n",
            "  Batch   280  of    611.    Elapsed: 0:06:08.\n",
            "  Batch   320  of    611.    Elapsed: 0:07:01.\n",
            "  Batch   360  of    611.    Elapsed: 0:07:53.\n",
            "  Batch   400  of    611.    Elapsed: 0:08:46.\n",
            "  Batch   440  of    611.    Elapsed: 0:09:39.\n",
            "  Batch   480  of    611.    Elapsed: 0:10:31.\n",
            "  Batch   520  of    611.    Elapsed: 0:11:24.\n",
            "  Batch   560  of    611.    Elapsed: 0:12:17.\n",
            "  Batch   600  of    611.    Elapsed: 0:13:09.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:13:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.90\n",
            "  Validation took: 0:00:41\n",
            "\n",
            "Training complete!\n",
            "Total training took 3:30:51 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KXEMvHJqsLL",
        "colab_type": "code",
        "outputId": "ca8883c4-8900-49ba-a5b6-844f380921f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.97e-01</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:24</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.66e-01</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:23</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.03e-01</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0:13:23</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.91e-02</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:23</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4.96e-02</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:22</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3.20e-02</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:22</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.55e-02</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:22</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.84e-02</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:23</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.11e-02</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0:13:22</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.16e-02</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:23</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6.76e-03</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:22</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>6.14e-03</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:23</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>7.64e-03</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:23</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4.41e-03</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:23</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3.25e-03</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0:13:23</td>\n",
              "      <td>0:00:41</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1           2.97e-01         0.26           0.90       0:13:24         0:00:41\n",
              "2           1.66e-01         0.30           0.90       0:13:23         0:00:41\n",
              "3           1.03e-01         0.44           0.89       0:13:23         0:00:41\n",
              "4           6.91e-02         0.46           0.90       0:13:23         0:00:41\n",
              "5           4.96e-02         0.53           0.90       0:13:22         0:00:41\n",
              "6           3.20e-02         0.64           0.90       0:13:22         0:00:41\n",
              "7           2.55e-02         0.69           0.90       0:13:22         0:00:41\n",
              "8           1.84e-02         0.74           0.90       0:13:23         0:00:41\n",
              "9           1.11e-02         0.80           0.89       0:13:22         0:00:41\n",
              "10          1.16e-02         0.82           0.90       0:13:23         0:00:41\n",
              "11          6.76e-03         0.87           0.90       0:13:22         0:00:41\n",
              "12          6.14e-03         0.88           0.90       0:13:23         0:00:41\n",
              "13          7.64e-03         0.88           0.90       0:13:23         0:00:41\n",
              "14          4.41e-03         0.89           0.90       0:13:23         0:00:41\n",
              "15          3.25e-03         0.90           0.90       0:13:23         0:00:41"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGeVZjBy7vV7",
        "colab_type": "code",
        "outputId": "8f49c500-4deb-46db-a2e9-3ed8f162f91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "train_df['flair_value'] = train_df['flair_value'].apply(lambda x: x.lower())\n",
        "y_true = train_df.processed_emotion.values\n",
        "y_pred = train_df.flair_value.values\n",
        "#target_names = ['class 0', 'class 1', 'class 2']\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(accuracy_score(y_true, y_pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.61      0.48      0.53      9673\n",
            "    positive       0.58      0.70      0.63      9860\n",
            "\n",
            "    accuracy                           0.59     19533\n",
            "   macro avg       0.59      0.59      0.58     19533\n",
            "weighted avg       0.59      0.59      0.58     19533\n",
            "\n",
            "0.5870066042082629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FE6lyZf12go",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "96c62216-7066-4930-f33d-9a60ad512905"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usiq8J59J6Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the DataLoader for Test set.\n",
        "prediction_data = dataset_test\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzd8YxUv2ZxQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "13dd3b5c-5828-4f21-b429-e21b0d8bf753"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids_test)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 2,542 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn3yQMVq2clL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "7a96515c-74d5-48b4-d9d4-6402184bcb9f"
      },
      "source": [
        "for i in range(len(predictions)):\n",
        "  if i == 0:\n",
        "    pred_flat = np.argmax(predictions[i], axis=1).flatten()\n",
        "    confidence_flat = np.amax(predictions[i], axis=1).flatten()\n",
        "    labels_flat = true_labels[i].flatten()\n",
        "    print(predictions[i])\n",
        "    print(confidence_flat)\n",
        "  else:\n",
        "    pred_flat = np.append(pred_flat, np.argmax(predictions[i], axis=1).flatten())\n",
        "    confidence_flat = np.append(confidence_flat, np.amax(predictions[i], axis=1).flatten())\n",
        "    labels_flat = np.append(labels_flat, true_labels[i].flatten())\n",
        "\n",
        "print(len(pred_flat), len(labels_flat))\n",
        "print(classification_report(labels_flat, pred_flat))\n",
        "print(accuracy_score(labels_flat, pred_flat))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5.5857515 -5.725335 ]\n",
            " [ 5.47231   -5.614704 ]\n",
            " [-5.6369643  5.8508744]\n",
            " [-5.5859647  5.8626122]\n",
            " [-5.5830846  5.7249947]\n",
            " [ 5.3411417 -5.341119 ]\n",
            " [ 5.4974604 -5.529719 ]\n",
            " [ 5.313862  -5.511909 ]\n",
            " [-5.690269   5.914673 ]\n",
            " [-5.337232   5.673433 ]\n",
            " [ 4.8303328 -5.365035 ]\n",
            " [ 5.098995  -5.5122976]\n",
            " [-5.3504124  5.635365 ]\n",
            " [-5.451075   5.705458 ]\n",
            " [-5.7685485  5.906415 ]\n",
            " [-5.6659765  5.8498726]\n",
            " [-5.700473   5.9068937]\n",
            " [ 5.4459395 -5.6214213]\n",
            " [ 5.156014  -5.1797037]\n",
            " [ 5.045055  -5.3748627]\n",
            " [-1.3112459  1.9447333]\n",
            " [-5.699914   5.921694 ]\n",
            " [-5.0996814  5.3286257]\n",
            " [ 5.4432073 -5.571834 ]\n",
            " [ 5.468812  -5.6143928]\n",
            " [-5.6639633  5.797509 ]\n",
            " [ 4.339024  -4.781442 ]\n",
            " [ 4.7862616 -4.850921 ]\n",
            " [-4.5241494  4.88408  ]\n",
            " [-4.2875347  4.6351185]\n",
            " [-5.7013     5.8761296]\n",
            " [ 5.502531  -5.562279 ]]\n",
            "[5.5857515 5.47231   5.8508744 5.8626122 5.7249947 5.3411417 5.4974604\n",
            " 5.313862  5.914673  5.673433  4.8303328 5.098995  5.635365  5.705458\n",
            " 5.906415  5.8498726 5.9068937 5.4459395 5.156014  5.045055  1.9447333\n",
            " 5.921694  5.3286257 5.4432073 5.468812  5.797509  4.339024  4.7862616\n",
            " 4.88408   4.6351185 5.8761296 5.502531 ]\n",
            "2542 2542\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.91      0.90      1227\n",
            "           1       0.91      0.90      0.91      1315\n",
            "\n",
            "    accuracy                           0.91      2542\n",
            "   macro avg       0.91      0.91      0.91      2542\n",
            "weighted avg       0.91      0.91      0.91      2542\n",
            "\n",
            "0.9067663257277734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEA7h3x5mGZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6c340001-cce8-46c1-8ee7-d73e056e9144"
      },
      "source": [
        "print('Positive samples in train: %d of %d (%.2f%%)' % (train_df.label.sum(), len(train_df.label), (train_df.label.sum() / len(train_df.label) * 100.0)))\n",
        "print('Positive samples in valid: %d of %d (%.2f%%)' % (valid_df.label.sum(), len(valid_df.label), (valid_df.label.sum() / len(valid_df.label) * 100.0)))\n",
        "print('Positive samples in test: %d of %d (%.2f%%)' % (test_df.label.sum(), len(test_df.label), (test_df.label.sum() / len(test_df.label) * 100.0)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples in train: 9860 of 19533 (50.48%)\n",
            "Positive samples in valid: 1386 of 2763 (50.16%)\n",
            "Positive samples in test: 1315 of 2542 (51.73%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}