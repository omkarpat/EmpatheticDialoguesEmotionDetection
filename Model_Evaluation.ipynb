{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Evaluation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrEb9ZNoHudSkHf/M9cjoe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "78576b98b69a4840af4fe54912abab86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2150770a5d2f450589a737d4a4944c54",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b4ee1c8ec4bc47f9b8fddd9da1772926",
              "IPY_MODEL_164a1a53c83f4b6da394b956bf1607fc"
            ]
          }
        },
        "2150770a5d2f450589a737d4a4944c54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4ee1c8ec4bc47f9b8fddd9da1772926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7b39c8cfa235438ea53299ae247df085",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7d78bcf565664889b53278d0b16c951c"
          }
        },
        "164a1a53c83f4b6da394b956bf1607fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_975e499cd4834cab9aed1e745deef7a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 383kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7212f77c875a452099876b9880a8bdc3"
          }
        },
        "7b39c8cfa235438ea53299ae247df085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7d78bcf565664889b53278d0b16c951c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "975e499cd4834cab9aed1e745deef7a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7212f77c875a452099876b9880a8bdc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omkarpat/EmpatheticDialoguesEmotionDetection/blob/master/Model_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKC2EjlunupU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "2fdb4630-cd4f-4989-c0b1-61a1598ba024"
      },
      "source": [
        "# Colab settings/mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My\\ Drive/CSE\\ 245\\ Project"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/CSE 245 Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3dPyPP9n9Gg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "8779c9df-fe84-44bc-cff1-79a753eecd81"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/b5/ac41e3e95205ebf53439e4dd087c58e9fd371fd8e3724f2b9b4cdb8282e5/transformers-2.10.0-py3-none-any.whl (660kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 14.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=09d785d67665ef646b47dd2433e6f02e442edd8fe39f49b0d8107d68ffccdcb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pICnJ8zoBqf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "098e86d7-1e55-4ed6-f11c-9fb252ec96f0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data\t\t     emotion_model3_save  model2_save  pos_neg_model4_save\n",
            "emotion_model1_save  emotion_model4_save  model3_save\n",
            "emotion_model2_save  model1_save\t  model4_save\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seESJAXDoHSP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "4f488ae8-1a71-4420-b50c-a28804a60d0f"
      },
      "source": [
        "!ls Data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Data Exploration.ipynb'\n",
            " data_fixed_train.json\n",
            " data_sample_100.json\n",
            " data_sample_10.json\n",
            " data_sample_10_processed.json\n",
            " data_sample_fixed_processed_final.json\n",
            " data_sample_fixed_processed_model1_final.csv\n",
            " data_sample_fixed_processed_model2_final.csv\n",
            " data_sample_fixed_processed_model3_final.csv\n",
            " data_sample_fixed_processed_model4_final.csv\n",
            " fixed\n",
            " fixed_test.csv\n",
            " fixed_test.json\n",
            " fixed_train_516.csv\n",
            " fixed_valid.csv\n",
            " fixed_valid.json\n",
            "'informative words.ipynb'\n",
            " Raw\n",
            " test_fixed_processed_final.json\n",
            " test_fixed_processed_model1_final.csv\n",
            " test_fixed_processed_model2_final.csv\n",
            " test_fixed_processed_model3_final.csv\n",
            " test_fixed_processed_model4_final.csv\n",
            " valid.csv\n",
            " valid_fixed_processed_final.json\n",
            " valid_fixed_processed_model1_final.csv\n",
            " valid_fixed_processed_model2_final.csv\n",
            " valid_fixed_processed_model3_final.csv\n",
            " valid_fixed_processed_model4_final.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0cbKcO1oKfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"model1\"\n",
        "model_type = \"emotion\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqAW4wudoxTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b0f72f99-6bac-4395-d5fb-38f8a0278058"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6fkdwMwpI5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "train_filename = \"Data/data_sample_fixed_processed_\" + model_name + \"_final.csv\"\n",
        "valid_filename = \"Data/valid_fixed_processed_\" + model_name + \"_final.csv\"\n",
        "test_filename = \"Data/test_fixed_processed_\" + model_name + \"_final.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_filename, sep='|', error_bad_lines=False)\n",
        "train_df = train_df.dropna()\n",
        "valid_df = pd.read_csv(valid_filename, sep='|', error_bad_lines=False)\n",
        "valid_df = valid_df.dropna()\n",
        "test_df = pd.read_csv(test_filename, sep='|', error_bad_lines=False)\n",
        "test_df = test_df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifj5qHOPqKNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category2index = {\n",
        "    \"invalid\":0,\n",
        "    \"sentimental\":1,\n",
        "    \"sad\":2,\n",
        "    \"hopeful\":3,\n",
        "    \"joyful\":4,\n",
        "    \"impressed\":5,\n",
        "    \"terrified\":6,\n",
        "    \"proud\":7,\n",
        "    \"afraid\":8,\n",
        "    \"grateful\":9,\n",
        "    \"lonely\":10,\n",
        "    \"surprised\":11,\n",
        "    \"anxious\":12,\n",
        "    \"guilty\":13,\n",
        "    \"trusting\":14,\n",
        "    \"nostalgic\":15,\n",
        "    \"excited\":16,\n",
        "    \"disgusted\":17,\n",
        "    \"caring\":18,\n",
        "    \"furious\":19,\n",
        "    \"angry\":20,\n",
        "    \"prepared\":21,\n",
        "    \"disappointed\":22,\n",
        "    \"faithful\":23,\n",
        "    \"confident\":24,\n",
        "    \"apprehensive\":25,\n",
        "    \"jealous\":26,\n",
        "    \"embarrassed\":27,\n",
        "    \"devastated\":28,\n",
        "    \"annoyed\":29,\n",
        "    \"ashamed\":30,\n",
        "    \"content\":31,\n",
        "    \"anticipating\":32\n",
        "}\n",
        "index2category = {}\n",
        "for key, value in category2index.items():\n",
        "  index2category[value] = key\n",
        "\n",
        "def category2label(category):\n",
        "  for key in category2index.keys():\n",
        "    if key == category:\n",
        "      return category2index[key]\n",
        "  return 0\n",
        "\n",
        "def label2emotion(labels):\n",
        "  emotion_list = []\n",
        "  for label in labels:\n",
        "    emotion_list.append(index2category[label])\n",
        "  return emotion_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC7JGNlyqQxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "6873cab3-e2d0-4296-a729-7e34ffd54000"
      },
      "source": [
        "print('Number of training sentences: {:,}\\n'.format(train_df.shape[0]))\n",
        "print('Number of valid sentences: {:,}\\n'.format(valid_df.shape[0]))\n",
        "print('Number of test sentences: {:,}\\n'.format(test_df.shape[0]))\n",
        "\n",
        "train_df['label'] = train_df['emotion'].apply(lambda x: int(category2label(x.lower())))\n",
        "valid_df['label'] = valid_df['emotion'].apply(lambda x: int(category2label(x.lower())))\n",
        "test_df['label'] = test_df['emotion'].apply(lambda x: int(category2label(x.lower())))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "train_df.sample(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 19,533\n",
            "\n",
            "Number of valid sentences: 2,763\n",
            "\n",
            "Number of test sentences: 2,542\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>processed_emotion</th>\n",
              "      <th>text</th>\n",
              "      <th>vader_neg</th>\n",
              "      <th>vader_neu</th>\n",
              "      <th>vader_pos</th>\n",
              "      <th>vader_compound</th>\n",
              "      <th>textblob</th>\n",
              "      <th>flair_value</th>\n",
              "      <th>flair_confidence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14830</th>\n",
              "      <td>lonely</td>\n",
              "      <td>negative</td>\n",
              "      <td>when my friends went out without me_comma_ and...</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.665</td>\n",
              "      <td>0.229</td>\n",
              "      <td>0.3931</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.978635</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17235</th>\n",
              "      <td>disappointed</td>\n",
              "      <td>negative</td>\n",
              "      <td>Once I planned an entire trip_comma_ went to b...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.156250</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.515797</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14149</th>\n",
              "      <td>content</td>\n",
              "      <td>positive</td>\n",
              "      <td>i farted infront of a pretty girl in a train once</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.686</td>\n",
              "      <td>0.314</td>\n",
              "      <td>0.4939</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.999959</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>devastated</td>\n",
              "      <td>negative</td>\n",
              "      <td>I made kimchi and it turned out pretty bad.</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.252</td>\n",
              "      <td>-0.0772</td>\n",
              "      <td>-0.225000</td>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.785030</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18536</th>\n",
              "      <td>sad</td>\n",
              "      <td>negative</td>\n",
              "      <td>My pet goldfish died recently. I was devastate...</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.396</td>\n",
              "      <td>0.175</td>\n",
              "      <td>-0.6712</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.983802</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2993</th>\n",
              "      <td>lonely</td>\n",
              "      <td>negative</td>\n",
              "      <td>i hate wen summer break is over. The house is ...</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.821</td>\n",
              "      <td>0.000</td>\n",
              "      <td>-0.5719</td>\n",
              "      <td>-0.266667</td>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.979788</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8368</th>\n",
              "      <td>hopeful</td>\n",
              "      <td>positive</td>\n",
              "      <td>After I applied for a job and had an interview...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.852</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.5106</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.731361</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19068</th>\n",
              "      <td>anxious</td>\n",
              "      <td>negative</td>\n",
              "      <td>I am scared about growing up_comma_ it is sort...</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.635</td>\n",
              "      <td>0.135</td>\n",
              "      <td>-0.2960</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.978605</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3959</th>\n",
              "      <td>impressed</td>\n",
              "      <td>positive</td>\n",
              "      <td>I was amazed at how beautiful San Sebastian wa...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.585</td>\n",
              "      <td>0.415</td>\n",
              "      <td>0.7964</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>NEGATIVE</td>\n",
              "      <td>0.994082</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17947</th>\n",
              "      <td>grateful</td>\n",
              "      <td>positive</td>\n",
              "      <td>I'm pregnant but wasn't able to have a baby sh...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.692</td>\n",
              "      <td>0.308</td>\n",
              "      <td>0.9109</td>\n",
              "      <td>0.307639</td>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.986484</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            emotion processed_emotion  ... flair_confidence  label\n",
              "14830        lonely          negative  ...         0.978635     10\n",
              "17235  disappointed          negative  ...         0.515797     22\n",
              "14149       content          positive  ...         0.999959     31\n",
              "407      devastated          negative  ...         0.785030     28\n",
              "18536           sad          negative  ...         0.983802      2\n",
              "2993         lonely          negative  ...         0.979788     10\n",
              "8368        hopeful          positive  ...         0.731361      3\n",
              "19068       anxious          negative  ...         0.978605     12\n",
              "3959      impressed          positive  ...         0.994082      5\n",
              "17947      grateful          positive  ...         0.986484      9\n",
              "\n",
              "[10 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPQeXgNmrpRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_train = train_df.text.values\n",
        "labels_train = train_df.label.values\n",
        "sentences_valid = valid_df.text.values\n",
        "labels_valid = valid_df.label.values\n",
        "sentences_test = test_df.text.values\n",
        "labels_test = test_df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ixqmd-SsFNa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "78576b98b69a4840af4fe54912abab86",
            "2150770a5d2f450589a737d4a4944c54",
            "b4ee1c8ec4bc47f9b8fddd9da1772926",
            "164a1a53c83f4b6da394b956bf1607fc",
            "7b39c8cfa235438ea53299ae247df085",
            "7d78bcf565664889b53278d0b16c951c",
            "975e499cd4834cab9aed1e745deef7a7",
            "7212f77c875a452099876b9880a8bdc3"
          ]
        },
        "outputId": "8a85ff15-face-415a-9bc0-d2f55404cc70"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "longer_list = []\n",
        "for index, sent in enumerate(sentences_train):\n",
        "  if len(tokenizer.encode(sent, add_special_tokens=True)) > 500:\n",
        "    longer_list.append(index)\n",
        "sentences_train = np.delete(sentences_train, longer_list)\n",
        "labels_train = np.delete(labels_train, longer_list)\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "def tokenize(sentences, labels):\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent in sentences:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length`\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = 256,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.    \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      \n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.from_numpy(labels)\n",
        "\n",
        "  return(input_ids, attention_masks, labels)\n",
        "\n",
        "input_ids_train, attention_masks_train, labels_train = tokenize(sentences_train, labels_train)\n",
        "input_ids_valid, attention_masks_valid, labels_valid = tokenize(sentences_valid, labels_valid)\n",
        "input_ids_test, attention_masks_test, labels_test = tokenize(sentences_test, labels_test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78576b98b69a4840af4fe54912abab86",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0SzIxf0s51S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4ce9a087-12a4-4a3d-b903-18bf585e0769"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
        "dataset_valid = TensorDataset(input_ids_valid, attention_masks_valid, labels_valid)\n",
        "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
        "\n",
        "#Use the given training and validation datasets\n",
        "train_dataset, val_dataset = dataset_train, dataset_valid\n",
        "\n",
        "print('{:>5,} training samples'.format(len(train_dataset)))\n",
        "print('{:>5,} validation samples'.format(len(val_dataset)))\n",
        "print('{:>5,} test samples'.format(len(dataset_test)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19,533 training samples\n",
            "2,763 validation samples\n",
            "2,542 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-Gydj5OtXt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55acca87-b7f2-4790-8494-d7ad62121c01"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "output_dir = \"./\" + model_type + \"_\" + model_name + \"_save/\"\n",
        "\n",
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
        "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)\n",
        "\n",
        "print(model_type + \"_\" + model_name + \" loaded...\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emotion_model1 loaded...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jHcY7NItPZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "2eff0647-33f9-4f36-fb0b-2a371b9b7759"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "batch_size = 32\n",
        "\n",
        "def predict(dataset):\n",
        "\n",
        "  # Create the DataLoader for  dataset.\n",
        "  prediction_data = dataset\n",
        "  prediction_sampler = SequentialSampler(prediction_data)\n",
        "  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "  # Prediction on test set\n",
        "\n",
        "  print('Predicting labels for {:,} sentences...'.format(len(dataset)))\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  predictions , true_labels = [], []\n",
        "\n",
        "  # Predict \n",
        "  for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "  print('DONE.')\n",
        "  return predictions, true_labels\n",
        "\n",
        "test_predictions, test_true_labels = predict(dataset_test)\n",
        "valid_predictions, valid_true_labels = predict(dataset_valid)\n",
        "train_predictions, train_true_labels = predict(dataset_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 2,542 sentences...\n",
            "DONE.\n",
            "Predicting labels for 2,763 sentences...\n",
            "DONE.\n",
            "Predicting labels for 19,533 sentences...\n",
            "DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntNnvi6kve9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7b9865c-55b5-425c-b6f6-ed12083a97c6"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def metrics(predictions, true_labels):\n",
        "  for i in range(len(predictions)):\n",
        "    if i == 0:\n",
        "      pred_flat = np.argmax(predictions[i], axis=1).flatten()\n",
        "      confidence_flat = np.amax(predictions[i], axis=1).flatten()\n",
        "      labels_flat = true_labels[i].flatten()\n",
        "    else:\n",
        "      pred_flat = np.append(pred_flat, np.argmax(predictions[i], axis=1).flatten())\n",
        "      confidence_flat = np.append(confidence_flat, np.amax(predictions[i], axis=1).flatten())\n",
        "      labels_flat = np.append(labels_flat, true_labels[i].flatten())\n",
        "\n",
        "  #print(len(pred_flat), len(labels_flat))\n",
        "  pred_flat = label2emotion(pred_flat)\n",
        "  labels_flat = label2emotion(labels_flat)\n",
        "  print(\"Classwise Report\")\n",
        "  print(classification_report(labels_flat, pred_flat))\n",
        "  return pred_flat, confidence_flat\n",
        "print(\"Metrics on testing set\")\n",
        "pred_flat_test, confidence_flat_test = metrics(test_predictions, test_true_labels)\n",
        "print(\"Metrics on validation set\")\n",
        "pred_flat_valid, confidence_flat_valid = metrics(valid_predictions, valid_true_labels)\n",
        "print(\"Metrics on training set\")\n",
        "pred_flat_train, confidence_flat_train = metrics(train_predictions, train_true_labels)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metrics on testing set\n",
            "Classwise Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      afraid       0.33      0.40      0.36        73\n",
            "       angry       0.32      0.29      0.30        84\n",
            "     annoyed       0.61      0.64      0.62        91\n",
            "anticipating       0.39      0.40      0.39        73\n",
            "     anxious       0.39      0.44      0.41        78\n",
            "apprehensive       0.43      0.41      0.42        71\n",
            "     ashamed       0.33      0.25      0.28        64\n",
            "      caring       0.72      0.60      0.65        80\n",
            "   confident       0.57      0.63      0.59        75\n",
            "     content       0.63      0.68      0.65        76\n",
            "  devastated       0.55      0.52      0.53        67\n",
            "disappointed       0.51      0.53      0.52        81\n",
            "   disgusted       0.69      0.80      0.74        86\n",
            " embarrassed       0.76      0.69      0.72        81\n",
            "     excited       0.45      0.43      0.44        91\n",
            "    faithful       0.67      0.66      0.67        50\n",
            "     furious       0.45      0.37      0.41        67\n",
            "    grateful       0.65      0.59      0.62        95\n",
            "      guilty       0.45      0.58      0.51        66\n",
            "     hopeful       0.60      0.44      0.51        79\n",
            "   impressed       0.64      0.65      0.65        81\n",
            "     jealous       0.85      0.54      0.66        81\n",
            "      joyful       0.36      0.43      0.39        83\n",
            "      lonely       0.73      0.83      0.78        78\n",
            "   nostalgic       0.59      0.73      0.65        77\n",
            "    prepared       0.74      0.65      0.69        77\n",
            "       proud       0.60      0.57      0.59        96\n",
            "         sad       0.45      0.51      0.48        88\n",
            " sentimental       0.42      0.31      0.36        91\n",
            "   surprised       0.61      0.55      0.58       127\n",
            "   terrified       0.41      0.54      0.46        71\n",
            "    trusting       0.51      0.62      0.56        64\n",
            "\n",
            "    accuracy                           0.54      2542\n",
            "   macro avg       0.54      0.54      0.54      2542\n",
            "weighted avg       0.55      0.54      0.54      2542\n",
            "\n",
            "Metrics on validation set\n",
            "Classwise Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      afraid       0.35      0.41      0.38        90\n",
            "       angry       0.23      0.26      0.25        84\n",
            "     annoyed       0.56      0.54      0.55       118\n",
            "anticipating       0.40      0.36      0.38        75\n",
            "     anxious       0.44      0.43      0.43        82\n",
            "apprehensive       0.51      0.46      0.48        74\n",
            "     ashamed       0.29      0.32      0.30        69\n",
            "      caring       0.69      0.69      0.69        75\n",
            "   confident       0.67      0.68      0.68        94\n",
            "     content       0.57      0.65      0.61        85\n",
            "  devastated       0.50      0.51      0.51        76\n",
            "disappointed       0.51      0.42      0.46        91\n",
            "   disgusted       0.74      0.77      0.75        82\n",
            " embarrassed       0.62      0.55      0.58        96\n",
            "     excited       0.47      0.46      0.47        97\n",
            "    faithful       0.62      0.50      0.56        50\n",
            "     furious       0.45      0.39      0.42        77\n",
            "    grateful       0.58      0.63      0.60        75\n",
            "      guilty       0.56      0.62      0.59        71\n",
            "     hopeful       0.53      0.53      0.53        81\n",
            "   impressed       0.63      0.59      0.61       103\n",
            "     jealous       0.74      0.73      0.73        89\n",
            "      joyful       0.39      0.44      0.41        85\n",
            "      lonely       0.83      0.79      0.81        95\n",
            "   nostalgic       0.66      0.75      0.70        83\n",
            "    prepared       0.74      0.71      0.73        97\n",
            "       proud       0.53      0.54      0.54        85\n",
            "         sad       0.45      0.48      0.46       101\n",
            " sentimental       0.52      0.45      0.48        82\n",
            "   surprised       0.62      0.61      0.61       148\n",
            "   terrified       0.44      0.48      0.46        82\n",
            "    trusting       0.62      0.63      0.63        71\n",
            "\n",
            "    accuracy                           0.55      2763\n",
            "   macro avg       0.55      0.54      0.54      2763\n",
            "weighted avg       0.55      0.55      0.55      2763\n",
            "\n",
            "Metrics on training set\n",
            "Classwise Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      afraid       0.98      0.97      0.98       634\n",
            "       angry       0.98      0.97      0.97       695\n",
            "     annoyed       0.99      0.99      0.99       667\n",
            "anticipating       0.97      0.98      0.98       606\n",
            "     anxious       1.00      0.98      0.99       614\n",
            "apprehensive       1.00      0.99      0.99       462\n",
            "     ashamed       0.99      0.98      0.98       493\n",
            "      caring       0.99      1.00      1.00       527\n",
            "   confident       0.99      1.00      0.99       616\n",
            "     content       1.00      0.99      0.99       576\n",
            "  devastated       0.99      0.99      0.99       569\n",
            "disappointed       1.00      1.00      1.00       598\n",
            "   disgusted       0.98      0.99      0.99       619\n",
            " embarrassed       0.99      0.99      0.99       563\n",
            "     excited       0.97      0.98      0.98       750\n",
            "    faithful       0.99      0.99      0.99       377\n",
            "     furious       0.98      0.98      0.98       615\n",
            "    grateful       1.00      1.00      1.00       645\n",
            "      guilty       0.98      1.00      0.99       622\n",
            "     hopeful       1.00      1.00      1.00       619\n",
            "   impressed       0.99      0.99      0.99       621\n",
            "     jealous       1.00      1.00      1.00       585\n",
            "      joyful       0.99      0.99      0.99       602\n",
            "      lonely       1.00      0.99      0.99       641\n",
            "   nostalgic       0.99      0.99      0.99       599\n",
            "    prepared       1.00      0.99      0.99       592\n",
            "       proud       0.99      0.99      0.99       686\n",
            "         sad       0.99      0.99      0.99       667\n",
            " sentimental       0.98      0.99      0.99       525\n",
            "   surprised       0.99      0.99      0.99      1004\n",
            "   terrified       0.97      0.98      0.98       629\n",
            "    trusting       1.00      0.99      1.00       515\n",
            "\n",
            "    accuracy                           0.99     19533\n",
            "   macro avg       0.99      0.99      0.99     19533\n",
            "weighted avg       0.99      0.99      0.99     19533\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}